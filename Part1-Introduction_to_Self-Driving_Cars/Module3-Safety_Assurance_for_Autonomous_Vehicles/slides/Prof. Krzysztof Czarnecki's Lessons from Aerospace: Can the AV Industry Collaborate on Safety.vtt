WEBVTT

1
00:00:13.580 --> 00:00:19.595
One of the questions that comes up often is how we legislate this technology.

2
00:00:19.595 --> 00:00:21.230
When there's an accident,

3
00:00:21.230 --> 00:00:23.480
eventually there will be and we've already seen

4
00:00:23.480 --> 00:00:27.530
some instances of early tests that have lead even to fatalities.

5
00:00:27.530 --> 00:00:30.260
But once we start deploying this more broadly,

6
00:00:30.260 --> 00:00:32.915
we'll never reach perfect safety records.

7
00:00:32.915 --> 00:00:35.180
We'll always have incidents that are still going to happen,

8
00:00:35.180 --> 00:00:37.625
failures that are going to occur that weren't predictable.

9
00:00:37.625 --> 00:00:41.450
So, how do we assign blame?

10
00:00:41.450 --> 00:00:43.520
Is the first incident where one of

11
00:00:43.520 --> 00:00:47.270
those car manufacturers vehicles takes out a school bus of children,

12
00:00:47.270 --> 00:00:51.665
is that going to end this entire industry?

13
00:00:51.665 --> 00:00:54.150
Are we going to be lost if

14
00:00:54.310 --> 00:00:57.300
lawsuits come out every month for

15
00:00:57.300 --> 00:01:00.315
these poor vehicles that are trying to drive on their own?

16
00:01:00.315 --> 00:01:08.870
The objective for the industry and everyone involved should be zero fatalities.

17
00:01:08.870 --> 00:01:10.310
That's the objective, right?

18
00:01:10.310 --> 00:01:10.835
Right.

19
00:01:10.835 --> 00:01:17.419
We know that any technical system can experience glitches,

20
00:01:17.419 --> 00:01:21.500
but we have to do everything possible to avoid

21
00:01:21.500 --> 00:01:26.000
such glitches that would lead to say fatalities or serious injuries. Right?

22
00:01:26.000 --> 00:01:26.160
Yeah.

23
00:01:26.160 --> 00:01:27.585
So, that's the first thing.

24
00:01:27.585 --> 00:01:29.350
That's the goal. Yeah.

25
00:01:29.360 --> 00:01:33.215
We can learn in the automotive industry a lot from

26
00:01:33.215 --> 00:01:38.790
Aerospace in a sense that Aerospace has put in place a system

27
00:01:38.790 --> 00:01:44.690
where all aspects of engineering these systems and operation and

28
00:01:44.690 --> 00:01:51.040
maintenance are subject to overall safety system,

29
00:01:51.040 --> 00:01:57.815
safety engineering where essentially you look at all the important risk factors.

30
00:01:57.815 --> 00:02:00.170
For example, I can give you an example of airlines,

31
00:02:00.170 --> 00:02:07.575
would look at say 100 risk factors and address all of them systematically.

32
00:02:07.575 --> 00:02:09.590
That includes both the technical aspect,

33
00:02:09.590 --> 00:02:12.710
but also pilot training and making sure that they

34
00:02:12.710 --> 00:02:16.550
slept enough and all these aspects, right? So, we can learn a lot.

35
00:02:16.550 --> 00:02:20.240
Yes. But aerospace only gets us so far in that they really have

36
00:02:20.240 --> 00:02:24.289
a much simpler and more open environment that they're operating in,

37
00:02:24.289 --> 00:02:27.080
and the real challenges for autonomous driving seem to

38
00:02:27.080 --> 00:02:29.950
come from the interaction with all of these other vehicles,

39
00:02:29.950 --> 00:02:31.795
and agents, and pedestrians.

40
00:02:31.795 --> 00:02:34.815
So, the complexity seems a completely different skill.

41
00:02:34.815 --> 00:02:42.380
So, you're absolutely right that the problem is different in reaction times,

42
00:02:42.380 --> 00:02:45.335
let's say when you fly an airplane,

43
00:02:45.335 --> 00:02:49.775
you have much more time to react to problems than in vehicles.

44
00:02:49.775 --> 00:02:52.160
What I was referring to,

45
00:02:52.160 --> 00:02:56.375
it's essentially this overall framework where every crash gets investigated,

46
00:02:56.375 --> 00:03:00.500
where every incident gets documented and we always learn from that and that

47
00:03:00.500 --> 00:03:04.955
learning is shared among all key stakeholders internationally.

48
00:03:04.955 --> 00:03:05.615
Wonderful.

49
00:03:05.615 --> 00:03:08.690
Basically, there's the idea

50
00:03:08.690 --> 00:03:14.910
that various players don't compete on safety and so that's the difference.

51
00:03:14.910 --> 00:03:19.580
So, we cannot take the model let's say in terms of individual may be

52
00:03:19.580 --> 00:03:24.425
concrete methods and techniques 100 percent,

53
00:03:24.425 --> 00:03:27.905
because let's say in aerospace you have a pilot who is

54
00:03:27.905 --> 00:03:32.420
highly trained and essentially the automation does all the easy medium staff,

55
00:03:32.420 --> 00:03:34.865
pilot is still there for all the hard stuff.

56
00:03:34.865 --> 00:03:39.755
In our case, we want to essentially automate everything.

57
00:03:39.755 --> 00:03:40.675
Yeah.

58
00:03:40.675 --> 00:03:46.200
We cannot rely that there is basically qualified in how they train pilots.

59
00:03:46.200 --> 00:03:47.930
So, these things have to be addressed,

60
00:03:47.930 --> 00:03:49.150
so that's very challenging.

61
00:03:49.150 --> 00:03:53.540
But the approach to safety can be essentially the same

62
00:03:53.540 --> 00:03:59.220
and it's required that we actually think about that and put this in place.