WEBVTT

1
00:00:00.000 --> 00:00:10.000
[MUSIC]

2
00:00:14.772 --> 00:00:16.020
So far in this module,

3
00:00:16.020 --> 00:00:19.903
we have reviewed what comprises
a feedforward neural network model,

4
00:00:19.903 --> 00:00:25.060
and how to evaluate the performance of a
neural network model using loss functions.

5
00:00:25.060 --> 00:00:28.860
This lesson will explain the final major
component of designing neural networks,

6
00:00:28.860 --> 00:00:30.770
the training process.

7
00:00:30.770 --> 00:00:34.258
Specifically, we will be
answering the following question.

8
00:00:34.258 --> 00:00:37.071
How can we get the best
parameter set theta for

9
00:00:37.071 --> 00:00:40.342
a feedforward neural network
given training data.

10
00:00:40.342 --> 00:00:44.112
The answer lies in using an iterative
optimization procedure with proper

11
00:00:44.112 --> 00:00:46.560
parameter initialization.

12
00:00:46.560 --> 00:00:50.700
Let us first revisit the feedforward
neural network training procedure

13
00:00:50.700 --> 00:00:52.540
we described previously.

14
00:00:52.540 --> 00:00:56.236
Given a training data input x and
the corresponding correct output,

15
00:00:56.236 --> 00:01:01.029
f star of x, we first pass the input
x through the hidden layers,

16
00:01:02.050 --> 00:01:05.720
then through the output layer
to get the final output y.

17
00:01:05.720 --> 00:01:09.162
We see here that the output y is
a function of the parameters theta.

18
00:01:09.162 --> 00:01:12.388
And remember,
that theta comprises the weights and

19
00:01:12.388 --> 00:01:16.282
the biases of our affine
transformations inside the network.

20
00:01:16.282 --> 00:01:20.678
Next, we compare our predicted output f
of x and theta with the correct output,

21
00:01:20.678 --> 00:01:23.580
f star of x through the loss function.

22
00:01:23.580 --> 00:01:28.524
Remember that the loss function measures
how large the error is between the network

23
00:01:28.524 --> 00:01:30.229
output and our true output.

24
00:01:30.229 --> 00:01:35.420
Our goal is to get a small value for the
loss function across the entire data set.

25
00:01:35.420 --> 00:01:40.068
We do so by using the loss function as
a guide to produce a new set of parameters

26
00:01:40.068 --> 00:01:44.154
theta that are expected to give
a lower value of the loss function.

27
00:01:44.154 --> 00:01:47.873
Specifically, we use the gradient
of the loss function to modify

28
00:01:47.873 --> 00:01:49.260
the parameters theta.

29
00:01:49.260 --> 00:01:52.890
This optimization procedure
is known as gradient descent.

30
00:01:52.890 --> 00:01:55.240
Before describing gradient
descent in detail,

31
00:01:55.240 --> 00:01:57.890
let's take another look at
the neural network loss function.

32
00:01:57.890 --> 00:02:01.430
Usually, we have thousands
of training example pairs,

33
00:02:01.430 --> 00:02:05.129
x and f star of x, available for
autonomous driving tasks.

34
00:02:05.129 --> 00:02:07.833
We can compute the loss
over all training examples,

35
00:02:07.833 --> 00:02:11.820
as the mean of the losses over
the individual training examples.

36
00:02:11.820 --> 00:02:15.022
We can then compute the gradient
of the training loss with

37
00:02:15.022 --> 00:02:19.088
respect to the parameters theta which
is equal to the mean of the gradient

38
00:02:19.088 --> 00:02:22.235
of the individual losses
over every training example.

39
00:02:22.235 --> 00:02:26.870
Here we use the fact that the gradient and
the sum are linear operators.

40
00:02:26.870 --> 00:02:30.910
So the gradient of a sum is equal to
the sum of the individual gradients.

41
00:02:30.910 --> 00:02:33.180
Using the formulated gradient equation,

42
00:02:33.180 --> 00:02:37.280
we can now describe the batch gradient
descent optimization algorithm.

43
00:02:37.280 --> 00:02:41.408
Batch gradient descent is a linear
first order optimization method.

44
00:02:41.408 --> 00:02:45.513
Iterative means that it starts from
an initial guess of parameters theta and

45
00:02:45.513 --> 00:02:48.510
improves on these parameters iteratively.

46
00:02:48.510 --> 00:02:52.610
First order means that the algorithm
only uses the first order derivative to

47
00:02:52.610 --> 00:02:54.410
improve the parameters theta.

48
00:02:54.410 --> 00:02:56.737
Batch gradient descent goes as follows.

49
00:02:56.737 --> 00:03:00.250
First, the parameters theta of
the neural network are initialized.

50
00:03:00.250 --> 00:03:03.152
Second, a stopping
condition is determined,

51
00:03:03.152 --> 00:03:07.514
which terminates the algorithm and
returns a final set of parameters.

52
00:03:07.514 --> 00:03:09.716
Once the iterative procedure begins,

53
00:03:09.716 --> 00:03:13.985
the first thing to be performed by the
algorithm is to compute the gradient of

54
00:03:13.985 --> 00:03:18.900
the loss function with respect to the
parameters theta, denoted del sub theta.

55
00:03:18.900 --> 00:03:22.989
The gradient can be computed using
the equation we derived earlier.

56
00:03:22.989 --> 00:03:27.168
Finally, the parameters theta are updated
according to the computed gradient.

57
00:03:27.168 --> 00:03:31.198
Here, epsilon is called the learning
rate and controls how much we adjust

58
00:03:31.198 --> 00:03:36.050
the parameters in the direction of
the negative gradient at every iteration.

59
00:03:36.050 --> 00:03:40.663
Let's take a look at a visual example of
batch gradient descent in the 2D case.

60
00:03:40.663 --> 00:03:44.031
Here, we are trying to find
the parameters theta one and

61
00:03:44.031 --> 00:03:47.480
theta two that minimize
our function J of theta.

62
00:03:47.480 --> 00:03:53.085
Theta is shaped like an oblong ball shown
here with contour lines of equal value.

63
00:03:53.085 --> 00:03:57.509
Gradient descent iteratively finds new
parameters theta that take us a step down

64
00:03:57.509 --> 00:03:59.530
the bowl at each iteration.

65
00:03:59.530 --> 00:04:03.380
The first step of the algorithm is
to initialize the parameters theta.

66
00:04:03.380 --> 00:04:06.990
Using our initial parameters,
we arrive at an initial value for

67
00:04:06.990 --> 00:04:10.070
our loss function denoted by the red dot.

68
00:04:10.070 --> 00:04:13.640
We start gradient descent by computing
the gradient of the loss function

69
00:04:13.640 --> 00:04:16.740
at the initial parameter
values theta 1 and theta 2.

70
00:04:16.740 --> 00:04:18.190
Using the update step,

71
00:04:18.190 --> 00:04:22.450
we then get the new parameters to arrive
at a lower point on our loss function.

72
00:04:22.450 --> 00:04:25.736
We repeat this process until we
achieve our stopping criteria.

73
00:04:25.736 --> 00:04:28.884
We then get the last set of
the parameters, theta 1 and

74
00:04:28.884 --> 00:04:33.030
theta 2 as our optimal set that
minimizes our loss function.

75
00:04:33.030 --> 00:04:35.842
Two pieces are still missing
from the presented algorithm.

76
00:04:35.842 --> 00:04:38.612
How do we initialize
the parameter's data and

77
00:04:38.612 --> 00:04:41.814
how do we decide when to
actually stop the algorithm?

78
00:04:41.814 --> 00:04:45.572
The answer to both of these questions is
still highly based on heuristics that work

79
00:04:45.572 --> 00:04:47.140
well in practice.

80
00:04:47.140 --> 00:04:51.996
For parameter initialization, we usually
initialized the weights using a standard

81
00:04:51.996 --> 00:04:54.633
normal distribution and
set the biases to 0.

82
00:04:54.633 --> 00:04:58.119
It is worth mentioning that there
are other heuristics specific to certain

83
00:04:58.119 --> 00:05:01.590
activation functions that
are widely used in a literature.

84
00:05:01.590 --> 00:05:04.580
We provide some of these heuristics
in a supplementary material.

85
00:05:04.580 --> 00:05:08.950
Defining the gradient descent's stopping
conditions is a bit more complex.

86
00:05:08.950 --> 00:05:12.880
There are three ways to determine
when to stop the training algorithm.

87
00:05:12.880 --> 00:05:16.090
Most simply,
we can decide to stop when a predefined

88
00:05:16.090 --> 00:05:19.440
maximum number of gradient
descent iterations is reached.

89
00:05:19.440 --> 00:05:23.090
Another heuristic is based on
how much the parameters theta

90
00:05:23.090 --> 00:05:25.010
changed between iterations.

91
00:05:25.010 --> 00:05:28.750
A small variation means the algorithm is
not updating the parameters effectively

92
00:05:28.750 --> 00:05:32.250
anymore, which might mean that
a minimum has been reached.

93
00:05:32.250 --> 00:05:36.281
The last widely used stopping criteria
is the change in the loss function value

94
00:05:36.281 --> 00:05:37.450
between iterations.

95
00:05:37.450 --> 00:05:41.527
Again, as the changes in the loss
function between iterations become small,

96
00:05:41.527 --> 00:05:45.260
the optimization is likely to
have converged to a minimum.

97
00:05:45.260 --> 00:05:48.450
Choosing one of these stopping
conditions is very much a matter of what

98
00:05:48.450 --> 00:05:50.410
works best for the problem at hand.

99
00:05:50.410 --> 00:05:53.964
We will revisit the stopping
conditions in the next lesson,

100
00:05:53.964 --> 00:05:58.642
as we study some of the pitfalls of the
training process, and how to avoid them.

101
00:05:58.642 --> 00:06:03.940
Unfortunately, the batch gradient descent
algorithm suffers from severe drawbacks.

102
00:06:03.940 --> 00:06:07.759
To be able to compute the gradient
we use backpropogation.

103
00:06:07.759 --> 00:06:11.852
Backpropogation involves computing
the output of the network for

104
00:06:11.852 --> 00:06:15.586
the example on which we would
like to evaluate the gradient.

105
00:06:15.586 --> 00:06:20.430
And batch gradient descent evaluates
the gradient over the whole training set.

106
00:06:20.430 --> 00:06:23.977
Making it very slow to
perform a single update step.

107
00:06:23.977 --> 00:06:28.512
Luckily, the laws function as well as
its gradient are means over the training

108
00:06:28.512 --> 00:06:29.630
dataset.

109
00:06:29.630 --> 00:06:34.346
For example, we know that the standard
error in a mean estimated from

110
00:06:34.346 --> 00:06:37.856
a set of N samples is sigma
over the square root of N.

111
00:06:37.856 --> 00:06:41.607
Where sigma is the standard
deviation of the distribution and

112
00:06:41.607 --> 00:06:44.791
N as the number of samples
used to estimate the mean.

113
00:06:44.791 --> 00:06:49.416
That means that the rate of decrease in
error in the gradient estimate is less

114
00:06:49.416 --> 00:06:51.970
than linear in the number of samples.

115
00:06:51.970 --> 00:06:55.890
This observation is very important,
as we now can use a small sub-sample

116
00:06:55.890 --> 00:06:59.480
of the training data or a mini batch
to compute our gradient estimate.

117
00:06:59.480 --> 00:07:03.380
So how does using mini batches modify
our batch gradient descent algorithm?

118
00:07:03.380 --> 00:07:05.800
The modification is actually quite simple.

119
00:07:05.800 --> 00:07:10.120
The only alteration to the base
algorithm is at the sampling step.

120
00:07:10.120 --> 00:07:15.030
Here we choose the sub sample n prime
of the training data as our mini batch.

121
00:07:15.030 --> 00:07:16.840
We can now evaluate the gradient and

122
00:07:16.840 --> 00:07:21.880
perform the update steps in an identical
manner to batch grading descent.

123
00:07:21.880 --> 00:07:25.750
This algorithm is called stochastic or
minibatch gradient descent,

124
00:07:25.750 --> 00:07:30.610
as we randomly select samples to include
in the minibatches at each iteration.

125
00:07:30.610 --> 00:07:34.840
However, this algorithm results in
an additional parameter to be determined,

126
00:07:34.840 --> 00:07:37.830
which is the size of the minibatch
that we want to use.

127
00:07:37.830 --> 00:07:42.080
To pick an appropriate minibatch, it has
to be noted that some kinds of hardware

128
00:07:42.080 --> 00:07:46.570
achieve better runtime with
specific sizes of data arrays.

129
00:07:46.570 --> 00:07:51.523
Specifically when using GPUs, it is common
to use power of two mini batch sizes

130
00:07:51.523 --> 00:07:55.366
which match GPU computing and
memory architecture as well.

131
00:07:55.366 --> 00:07:58.013
And therefore,
use the GPU resources efficiently.

132
00:07:58.013 --> 00:08:01.025
Let's look at some of the factors
that drive batch size selection.

133
00:08:01.025 --> 00:08:05.845
Multi-core architectures such as GPUs
are usually under-utilized by extremely

134
00:08:05.845 --> 00:08:10.593
small batch sizes, which motivates using
some absolute minimum batch size below

135
00:08:10.593 --> 00:08:14.341
which there's no reduction in
the time to process a minibatch.

136
00:08:14.341 --> 00:08:18.076
Furthermore, large batch sizes usually
provide a more accurate estimate of

137
00:08:18.076 --> 00:08:19.430
the gradient.

138
00:08:19.430 --> 00:08:22.190
Ensuring descent in a direction
that improves the network

139
00:08:22.190 --> 00:08:24.030
performance more reliably.

140
00:08:24.030 --> 00:08:25.672
However as noted previously,

141
00:08:25.672 --> 00:08:29.405
this improvement in the accuracy of
the estimate is less than linear.

142
00:08:29.405 --> 00:08:34.730
Small batch sizes on the other hand have
been seen to offer a regularlizing effect.

143
00:08:34.730 --> 00:08:38.710
With the best generalization often
seen at a batch size of one.

144
00:08:38.710 --> 00:08:42.490
If you're not sure what we mean
by generalization, don't worry.

145
00:08:42.490 --> 00:08:45.187
As we'll be exploring it more
closely in the next lesson.

146
00:08:45.187 --> 00:08:49.919
Furthermore, optimization algorithms
usually converge more quickly if they're

147
00:08:49.919 --> 00:08:54.386
allowed to rapidly compute approximate
estimates of the gradients and iterate

148
00:08:54.386 --> 00:09:00.140
more often rather than computing exact
gradients and performing fewer iterations.

149
00:09:00.140 --> 00:09:05.128
As a result of these trade-offs, typical
power of two mini batch sizes range

150
00:09:05.128 --> 00:09:09.733
from 32 to 256, with smaller sizes
sometimes being attempted for

151
00:09:09.733 --> 00:09:12.666
large models or to improve generalization.

152
00:09:12.666 --> 00:09:16.592
One final issue to keep in mind is the
requirement to shuffle the dataset before

153
00:09:16.592 --> 00:09:18.640
sampling the minibatch.

154
00:09:18.640 --> 00:09:23.140
Failing to shuffle the dataset at all can
reduce the effectiveness of your network.

155
00:09:23.140 --> 00:09:27.142
There exist many variants of stochastic
gradient descent in the literature,

156
00:09:27.142 --> 00:09:30.590
each having their own advantages and
disadvantages.

157
00:09:30.590 --> 00:09:33.630
It might be difficult to choose
which variant to use, and

158
00:09:33.630 --> 00:09:37.580
sometimes one of the variants works
better for certain problem than another.

159
00:09:37.580 --> 00:09:39.000
As a simple rule of thumb for

160
00:09:39.000 --> 00:09:44.080
autonomous driving applications, a safe
choice is the ADAM optimization method.

161
00:09:44.080 --> 00:09:48.847
It is quite robust to initial
parameters theta, and widely use.

162
00:09:48.847 --> 00:09:52.006
If you are interest in learning
more about this variance,

163
00:09:52.006 --> 00:09:55.432
have a look at the resources
listed in the supplemental notes.

164
00:09:55.432 --> 00:09:58.171
In this lesson,
you learned how to optimize the parameters

165
00:09:58.171 --> 00:10:01.300
of a neural network using
batch gradient descent.

166
00:10:01.300 --> 00:10:05.905
You also learned that there are a lot of
proposed variance of this optimization

167
00:10:05.905 --> 00:10:08.989
algorithm, with a safety
fault choice being ADAM.

168
00:10:08.989 --> 00:10:12.822
Congratulations, you've finished
the essential steps required to build and

169
00:10:12.822 --> 00:10:14.560
train an neural network.

170
00:10:14.560 --> 00:10:19.114
In the next lesson, we will discuss how
you can choose some of the optimization

171
00:10:19.114 --> 00:10:23.192
parameters to improve network training,
such as the learning rate.

172
00:10:23.192 --> 00:10:27.825
Also we'll discuss how to evaluate the
performance of our neural network using

173
00:10:27.825 --> 00:10:29.100
validation sets.

174
00:10:29.100 --> 00:10:30.019
See you next time.

175
00:10:30.019 --> 00:10:39.261
[MUSIC]