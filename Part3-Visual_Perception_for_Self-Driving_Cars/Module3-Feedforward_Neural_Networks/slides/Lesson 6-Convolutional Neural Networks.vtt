WEBVTT

1
00:00:13.400 --> 00:00:15.480
If you've been monitoring

2
00:00:15.480 --> 00:00:17.625
the latest news on
self-driving cars,

3
00:00:17.625 --> 00:00:18.360
you would have heard

4
00:00:18.360 --> 00:00:20.625
the phrase convolutional
neural networks

5
00:00:20.625 --> 00:00:23.730
or ConvNets for
short at least a few times.

6
00:00:23.730 --> 00:00:25.230
In fact, we currently use

7
00:00:25.230 --> 00:00:27.210
ConvNets to perform
a multitude of

8
00:00:27.210 --> 00:00:28.710
perception tasks on

9
00:00:28.710 --> 00:00:30.885
our own self-driving car
the autonomoose.

10
00:00:30.885 --> 00:00:33.240
In this lesson, we will
take a deeper look at

11
00:00:33.240 --> 00:00:35.290
these fascinating
architectures to

12
00:00:35.290 --> 00:00:38.335
understand their importance
for visual perception.

13
00:00:38.335 --> 00:00:40.460
Specifically, you will learn how

14
00:00:40.460 --> 00:00:43.325
convolutional layers
use cross-correlation

15
00:00:43.325 --> 00:00:45.710
instead of general
matrix multiplication

16
00:00:45.710 --> 00:00:48.920
to tailor neural networks
for image input data.

17
00:00:48.920 --> 00:00:51.500
We'll also cover
the advantages these models

18
00:00:51.500 --> 00:00:54.785
incur over standard
feed-forward neural networks.

19
00:00:54.785 --> 00:00:58.010
Convolutional neural networks
are a specialized kind of

20
00:00:58.010 --> 00:00:59.750
neural network for
processing data

21
00:00:59.750 --> 00:01:02.045
that has a known
grid-like topology.

22
00:01:02.045 --> 00:01:04.100
Examples of such data can be

23
00:01:04.100 --> 00:01:07.565
1D time-series data sampled
at regular intervals,

24
00:01:07.565 --> 00:01:10.685
2D images or even 3D videos.

25
00:01:10.685 --> 00:01:14.540
ConvNets are mainly comprised
of two types of layers;

26
00:01:14.540 --> 00:01:17.345
convolutional layers
and pooling layers.

27
00:01:17.345 --> 00:01:22.115
A simple example of a convNet
architecture is VGG 16.

28
00:01:22.115 --> 00:01:24.620
This network takes
in the image and

29
00:01:24.620 --> 00:01:27.770
passes it through a set
of convolutional layers,

30
00:01:27.770 --> 00:01:29.570
a pooling layer,

31
00:01:29.570 --> 00:01:32.165
and another couple of
convolutional layers,

32
00:01:32.165 --> 00:01:33.830
and then more pooling layers and

33
00:01:33.830 --> 00:01:35.825
convolutional layers and so on.

34
00:01:35.825 --> 00:01:38.120
Don't worry too much
about the specifics of

35
00:01:38.120 --> 00:01:40.540
the VGG 16 architecture
design for now,

36
00:01:40.540 --> 00:01:42.470
we will discuss
this architecture in

37
00:01:42.470 --> 00:01:43.910
detail in a later video

38
00:01:43.910 --> 00:01:45.950
when we learn about
object detection.

39
00:01:45.950 --> 00:01:49.580
Let's see how these two types
of layers work in practice.

40
00:01:49.580 --> 00:01:52.280
The neural network, hidden
layers we have described so

41
00:01:52.280 --> 00:01:55.520
far are usually called
fully connected layers.

42
00:01:55.520 --> 00:01:57.440
As their name suggests,

43
00:01:57.440 --> 00:01:59.345
fully connected layers connect

44
00:01:59.345 --> 00:02:03.215
each node output to every
node input in the next layer.

45
00:02:03.215 --> 00:02:05.660
That means that
every element of the input

46
00:02:05.660 --> 00:02:08.615
contributes to every element
of the output.

47
00:02:08.615 --> 00:02:10.610
This is implemented in software

48
00:02:10.610 --> 00:02:13.100
through dense matrix
multiplication.

49
00:02:13.100 --> 00:02:16.610
Although counter-intuitive,
convolutional layers use

50
00:02:16.610 --> 00:02:18.800
cross-correlation
not convolutions for

51
00:02:18.800 --> 00:02:20.675
their linear operator instead

52
00:02:20.675 --> 00:02:22.870
of general matrix multiplication.

53
00:02:22.870 --> 00:02:24.650
The logic behind using

54
00:02:24.650 --> 00:02:27.834
cross-correlation is that if
the parameters are learned,

55
00:02:27.834 --> 00:02:31.235
it does not matter if we
flip the output or not.

56
00:02:31.235 --> 00:02:32.930
Since we are learning the weights

57
00:02:32.930 --> 00:02:34.100
of the convolutional layer,

58
00:02:34.100 --> 00:02:37.070
the flipping does not
affect our results at all.

59
00:02:37.070 --> 00:02:39.965
This results in what we
call sparse connectivity.

60
00:02:39.965 --> 00:02:41.600
Each input element to

61
00:02:41.600 --> 00:02:43.040
the convolutional layer only

62
00:02:43.040 --> 00:02:44.810
affects a few output elements,

63
00:02:44.810 --> 00:02:45.920
thanks to the use of

64
00:02:45.920 --> 00:02:49.025
a limited size kernel for
the convolutional operation.

65
00:02:49.025 --> 00:02:50.810
Let's begin by describing how

66
00:02:50.810 --> 00:02:53.480
convolutional layers
work in practice.

67
00:02:53.480 --> 00:02:55.100
We'll assume that
we want to apply

68
00:02:55.100 --> 00:02:57.455
a convolutional layer
to an input image.

69
00:02:57.455 --> 00:03:00.830
We will refer to this image
as our input volume,

70
00:03:00.830 --> 00:03:03.260
as we will see
convolutional layers taking

71
00:03:03.260 --> 00:03:06.290
output of other layers
as their inputs as well.

72
00:03:06.290 --> 00:03:08.405
The width of our input volume

73
00:03:08.405 --> 00:03:10.400
is its horizontal dimension,

74
00:03:10.400 --> 00:03:12.980
the height is
its vertical dimension,

75
00:03:12.980 --> 00:03:15.485
and the depth is
the number of channels.

76
00:03:15.485 --> 00:03:18.050
In our case, all
three characteristics

77
00:03:18.050 --> 00:03:19.655
have a value of three.

78
00:03:19.655 --> 00:03:21.170
But why didn't we consider

79
00:03:21.170 --> 00:03:24.365
the gray pixels in our
height or width computation?

80
00:03:24.365 --> 00:03:26.000
The gray pixels are added to

81
00:03:26.000 --> 00:03:28.175
the image through
a process called padding.

82
00:03:28.175 --> 00:03:30.740
The number of pixels
added on each side is

83
00:03:30.740 --> 00:03:33.580
called the padding
size in this case one.

84
00:03:33.580 --> 00:03:35.690
Padding is essential
for retaining

85
00:03:35.690 --> 00:03:38.240
the shape required to
perform the convolutions.

86
00:03:38.240 --> 00:03:40.310
We perform the convolution
operations through

87
00:03:40.310 --> 00:03:42.740
a set of kernels or filters.

88
00:03:42.740 --> 00:03:45.020
Each Filter is
comprised of a set of

89
00:03:45.020 --> 00:03:47.840
weights and a single bias.

90
00:03:47.840 --> 00:03:50.480
The number of channels
of the kernel needs to

91
00:03:50.480 --> 00:03:53.390
correspond to the number of
channels of the input volume.

92
00:03:53.390 --> 00:03:54.890
In this case, we have

93
00:03:54.890 --> 00:03:58.180
three weight channels per
filter corresponding to red,

94
00:03:58.180 --> 00:04:00.785
green, and blue channels
of the input image.

95
00:04:00.785 --> 00:04:03.965
Usually we have multiple filters
per convolutional layer.

96
00:04:03.965 --> 00:04:06.560
Let's see how to
apply our two filters

97
00:04:06.560 --> 00:04:09.230
to get an output volume
from our input volume.

98
00:04:09.230 --> 00:04:12.440
We start by taking each channel
of the filter and perform

99
00:04:12.440 --> 00:04:14.705
cross-correlation
between that channel

100
00:04:14.705 --> 00:04:17.180
and its corresponding channel
in the input volume.

101
00:04:17.180 --> 00:04:18.620
We then proceed to add

102
00:04:18.620 --> 00:04:20.420
the output of
the cross-correlation of

103
00:04:20.420 --> 00:04:22.670
all channels with the bias

104
00:04:22.670 --> 00:04:25.520
of the filter to arrive at
the final output value.

105
00:04:25.520 --> 00:04:29.135
Notice that we get
one output channel per filter.

106
00:04:29.135 --> 00:04:31.355
We will get back to
this point in a bit.

107
00:04:31.355 --> 00:04:34.340
Let's now see how to get
the rest of the output volume.

108
00:04:34.340 --> 00:04:36.995
After we're done with
the first computation,

109
00:04:36.995 --> 00:04:38.840
we shift the filter location by

110
00:04:38.840 --> 00:04:41.255
a preset number of
pixels horizontally.

111
00:04:41.255 --> 00:04:43.939
When we reach the end of
the input volume width,

112
00:04:43.939 --> 00:04:45.590
we shift the filter location by

113
00:04:45.590 --> 00:04:47.945
a preset number of
pixels vertically.

114
00:04:47.945 --> 00:04:50.105
The vertical and
horizontal shifts

115
00:04:50.105 --> 00:04:51.725
are usually the same value,

116
00:04:51.725 --> 00:04:53.600
and we refer to this value as

117
00:04:53.600 --> 00:04:55.970
the stride of
our convolutional layer.

118
00:04:55.970 --> 00:04:59.820
We arrive to a final output
volume with its own width,

119
00:04:59.820 --> 00:05:01.605
depth, and height values.

120
00:05:01.605 --> 00:05:04.155
Assuming that
the filters are M by M,

121
00:05:04.155 --> 00:05:07.560
and we have K filters,
a stride of S,

122
00:05:07.560 --> 00:05:11.655
a padding P, we can derive
expressions for the width,

123
00:05:11.655 --> 00:05:14.945
height, and depth of
our output volume.

124
00:05:14.945 --> 00:05:18.080
You would think this gets
challenging to keep track of,

125
00:05:18.080 --> 00:05:19.805
but when designing ConvNets,

126
00:05:19.805 --> 00:05:21.440
it is very important to know

127
00:05:21.440 --> 00:05:24.080
what size output layers
you'll end up with.

128
00:05:24.080 --> 00:05:26.330
As an example, you don't want to

129
00:05:26.330 --> 00:05:28.715
reduce the size of
your output volume too much

130
00:05:28.715 --> 00:05:31.010
if you are trying to
detect small traffic signs

131
00:05:31.010 --> 00:05:33.605
and traffic lights
on road scenes.

132
00:05:33.605 --> 00:05:36.980
They only occupy a small number
of pixels in the image,

133
00:05:36.980 --> 00:05:38.720
and their visibility
might get lost if

134
00:05:38.720 --> 00:05:41.585
the output volume is too compact.

135
00:05:41.585 --> 00:05:44.090
Let us now continue to describe

136
00:05:44.090 --> 00:05:47.435
the second building block of
ConvNets, pooling layers.

137
00:05:47.435 --> 00:05:49.190
A pooling layer uses pooling

138
00:05:49.190 --> 00:05:51.080
functions to replace
the output of

139
00:05:51.080 --> 00:05:52.310
the previous layer with

140
00:05:52.310 --> 00:05:55.265
a summary statistic of
the nearby outputs.

141
00:05:55.265 --> 00:05:57.590
Pooling helps make
the representations

142
00:05:57.590 --> 00:06:00.865
become invariant to
small translations of the input.

143
00:06:00.865 --> 00:06:03.805
If we translate the input
a small amount,

144
00:06:03.805 --> 00:06:06.695
the output of the pooling layer
will not change.

145
00:06:06.695 --> 00:06:09.770
This is important for
object recognition for example,

146
00:06:09.770 --> 00:06:13.190
as if we shift a car a small
amount in an image space,

147
00:06:13.190 --> 00:06:15.785
it should still be
recognizable as a car.

148
00:06:15.785 --> 00:06:17.450
Let us take an example of

149
00:06:17.450 --> 00:06:20.935
the most commonly used
pooling layer, Max pooling.

150
00:06:20.935 --> 00:06:24.005
Max pooling summarizes
output volume patches

151
00:06:24.005 --> 00:06:25.535
with the max function.

152
00:06:25.535 --> 00:06:27.830
Given the input volume in gray,

153
00:06:27.830 --> 00:06:32.075
Max Pooling applies the max
function to an M by M region,

154
00:06:32.075 --> 00:06:33.770
then shifts this region in

155
00:06:33.770 --> 00:06:36.215
strides similar to the
convolutional layer.

156
00:06:36.215 --> 00:06:37.820
Once again we can derive

157
00:06:37.820 --> 00:06:39.875
expressions for the output width,

158
00:06:39.875 --> 00:06:41.810
height, and depth of

159
00:06:41.810 --> 00:06:45.395
the pooling layers according
to the following equations.

160
00:06:45.395 --> 00:06:47.305
In our previous example,

161
00:06:47.305 --> 00:06:49.740
the filter size M is two,

162
00:06:49.740 --> 00:06:51.330
and we get a stride of two,

163
00:06:51.330 --> 00:06:54.320
so we end up with
a two-by-two output.

164
00:06:54.320 --> 00:06:56.510
Let's see how this
pooling layer can help

165
00:06:56.510 --> 00:06:58.610
us with translation invariance.

166
00:06:58.610 --> 00:07:00.380
As an example, let's shift

167
00:07:00.380 --> 00:07:03.035
the previous input volume
by one pixel.

168
00:07:03.035 --> 00:07:06.709
The added pixels due to
the shift are shown in blue,

169
00:07:06.709 --> 00:07:08.975
whereas the removed pixels
are shown in red.

170
00:07:08.975 --> 00:07:10.520
We can go ahead and apply

171
00:07:10.520 --> 00:07:12.865
Max Pooling to this input volume,

172
00:07:12.865 --> 00:07:15.000
as we did in the previous slide.

173
00:07:15.000 --> 00:07:16.940
When comparing our new output to

174
00:07:16.940 --> 00:07:18.800
the original volume output,

175
00:07:18.800 --> 00:07:21.425
we find that only one
element has changed.

176
00:07:21.425 --> 00:07:23.090
So far, we've discussed how

177
00:07:23.090 --> 00:07:25.460
ConvNets operate
but still did not

178
00:07:25.460 --> 00:07:26.660
provide a reason for

179
00:07:26.660 --> 00:07:29.735
their usefulness in the context
of self-driving cars.

180
00:07:29.735 --> 00:07:31.805
There are really
two important reasons

181
00:07:31.805 --> 00:07:33.965
for the effectiveness
of ConvNets.

182
00:07:33.965 --> 00:07:37.220
First, they usually have
far fewer parameters in

183
00:07:37.220 --> 00:07:38.840
their convolutional layers than

184
00:07:38.840 --> 00:07:41.270
a similar network with
fully connected layers.

185
00:07:41.270 --> 00:07:42.590
This reduces the chance of

186
00:07:42.590 --> 00:07:44.870
over-fitting through
parameters sharing,

187
00:07:44.870 --> 00:07:48.260
and allows ConvNets to
operate on larger images.

188
00:07:48.260 --> 00:07:51.985
Perhaps more importantly,
is translation invariance.

189
00:07:51.985 --> 00:07:53.750
By using the same parameters to

190
00:07:53.750 --> 00:07:55.640
process every block of the image,

191
00:07:55.640 --> 00:07:56.990
ConvNets are capable of

192
00:07:56.990 --> 00:07:59.060
detecting an object
or classifying

193
00:07:59.060 --> 00:08:00.650
a pixel even if it is

194
00:08:00.650 --> 00:08:03.050
shifted with a translation
on the image plane.

195
00:08:03.050 --> 00:08:06.020
This means we can detect
cars wherever they appear.

196
00:08:06.020 --> 00:08:07.700
Before we end this lesson,

197
00:08:07.700 --> 00:08:08.990
I would like to shed light on

198
00:08:08.990 --> 00:08:11.285
the history of convolutional
neural networks.

199
00:08:11.285 --> 00:08:13.220
Convolutional neural
networks have played

200
00:08:13.220 --> 00:08:16.040
an important role in
the history of deep learning.

201
00:08:16.040 --> 00:08:17.900
As a matter of
fact, ConvNets were

202
00:08:17.900 --> 00:08:19.640
one of the first
neural network models

203
00:08:19.640 --> 00:08:21.275
to perform well at a time where

204
00:08:21.275 --> 00:08:23.390
other feed-forward
architectures failed,

205
00:08:23.390 --> 00:08:25.970
particularly on
image classification tasks

206
00:08:25.970 --> 00:08:27.620
related to the ImageNet dataset.

207
00:08:27.620 --> 00:08:29.785
In many ways, ConvNets carry

208
00:08:29.785 --> 00:08:31.910
the torch for the rest
of deep learning,

209
00:08:31.910 --> 00:08:33.170
and pave the way to

210
00:08:33.170 --> 00:08:34.850
the relatively new acceptance

211
00:08:34.850 --> 00:08:36.650
of neural networks in general.

212
00:08:36.650 --> 00:08:39.020
Finally, convolutional
networks were some of

213
00:08:39.020 --> 00:08:40.220
the first neural networks to

214
00:08:40.220 --> 00:08:42.550
solve important
commercial applications,

215
00:08:42.550 --> 00:08:43.850
the most famous being

216
00:08:43.850 --> 00:08:46.505
Yann LeCun's handwritten
digit recognizer

217
00:08:46.505 --> 00:08:47.989
in the early nineties,

218
00:08:47.989 --> 00:08:49.670
and remain at the forefront of

219
00:08:49.670 --> 00:08:52.505
commercial applications
of deep learning today.

220
00:08:52.505 --> 00:08:54.985
In fact, in the next week
of this course,

221
00:08:54.985 --> 00:08:57.170
we will show you how
to use ConvNets to

222
00:08:57.170 --> 00:09:00.175
detect a range of different
objects in roads scenes,

223
00:09:00.175 --> 00:09:03.410
2D object detection.
We'll see you then.