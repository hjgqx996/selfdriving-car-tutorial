WEBVTT

1
00:00:14.000 --> 00:00:18.060
Self-driving cars require
accurate depth perception

2
00:00:18.060 --> 00:00:20.660
for the safe operation of
our autonomous vehicles.

3
00:00:20.660 --> 00:00:21.980
If you don't know how far away

4
00:00:21.980 --> 00:00:23.300
the cars are in front of you,

5
00:00:23.300 --> 00:00:25.390
how can you avoid
them while driving?

6
00:00:25.390 --> 00:00:27.980
Lidar and Radar sensors
are usually thought of as

7
00:00:27.980 --> 00:00:31.550
the primary 3D sensors
available for perception tasks.

8
00:00:31.550 --> 00:00:34.070
However, we can get
depth information from

9
00:00:34.070 --> 00:00:37.550
two or more cameras using
multi-view geometry.

10
00:00:37.550 --> 00:00:40.235
Specifically, we'll be describing

11
00:00:40.235 --> 00:00:41.600
the process of getting depth

12
00:00:41.600 --> 00:00:43.355
from two axis aligned cameras

13
00:00:43.355 --> 00:00:45.875
a setup known as
the stereo cameras.

14
00:00:45.875 --> 00:00:48.710
In this video, we will
cover the geometry

15
00:00:48.710 --> 00:00:51.290
of the stereo sensor
as well as how

16
00:00:51.290 --> 00:00:54.350
to derive the 3D coordinates
of a point given

17
00:00:54.350 --> 00:00:57.995
its projection onto two images
of the stereo sensor.

18
00:00:57.995 --> 00:01:01.760
Stereopsis, the process
of stereo vision,

19
00:01:01.760 --> 00:01:05.645
was first described by
Charles Wheatstone back in 1838.

20
00:01:05.645 --> 00:01:07.175
He recognized that because

21
00:01:07.175 --> 00:01:09.320
each eye views
the visual world from

22
00:01:09.320 --> 00:01:11.645
a slightly different
horizontal position

23
00:01:11.645 --> 00:01:14.225
that each eye's image
differs from the other.

24
00:01:14.225 --> 00:01:16.160
Objects at different
distances from

25
00:01:16.160 --> 00:01:19.520
the eye project images into
the two eyes that differ in

26
00:01:19.520 --> 00:01:22.610
their horizontal position
giving depth cues of

27
00:01:22.610 --> 00:01:24.590
horizontal disparity that are

28
00:01:24.590 --> 00:01:26.660
also known as
binocular disparity.

29
00:01:26.660 --> 00:01:29.420
However, historical
evidence suggests

30
00:01:29.420 --> 00:01:32.135
that stereopsis was discovered
much earlier than this.

31
00:01:32.135 --> 00:01:34.805
In fact some drawings
by Leonardo da Vinci

32
00:01:34.805 --> 00:01:38.785
depict accurate geometry of
depth through stereopsis.

33
00:01:38.785 --> 00:01:40.425
Up to the 19th century,

34
00:01:40.425 --> 00:01:42.470
the phenomenon of stereopsis was

35
00:01:42.470 --> 00:01:44.540
primarily used for entertainment.

36
00:01:44.540 --> 00:01:46.700
Anaglyphs were used to provide

37
00:01:46.700 --> 00:01:50.270
a stereoscopic 3D effect when
viewed with 2D color glass,

38
00:01:50.270 --> 00:01:51.980
where each lens employs

39
00:01:51.980 --> 00:01:54.185
different chromatically
opposite colors,

40
00:01:54.185 --> 00:01:56.105
usually red and cyan.

41
00:01:56.105 --> 00:01:58.489
Nowadays, we use stereopsis

42
00:01:58.489 --> 00:02:00.230
with complex algorithms to derive

43
00:02:00.230 --> 00:02:02.180
depth from two images using

44
00:02:02.180 --> 00:02:04.870
a similar concept to
Da Vincis drawings.

45
00:02:04.870 --> 00:02:08.260
Now, let us delve into
the geometry of a stereo sensor.

46
00:02:08.260 --> 00:02:10.430
A stereo sensor is
usually created by

47
00:02:10.430 --> 00:02:13.265
two cameras with
parallel optical axes.

48
00:02:13.265 --> 00:02:14.965
To simplify
the problem even more,

49
00:02:14.965 --> 00:02:18.710
most manufacturers align
the cameras in 3D space so

50
00:02:18.710 --> 00:02:20.630
that the two image
planes are aligned with

51
00:02:20.630 --> 00:02:22.730
only an offset in the x-axis.

52
00:02:22.730 --> 00:02:25.399
Given a known rotation
and translation

53
00:02:25.399 --> 00:02:28.445
between the two cameras
and a known projection

54
00:02:28.445 --> 00:02:31.760
of a point O in 3D to
the two camera frames

55
00:02:31.760 --> 00:02:36.200
resulting in pixel locations
OL and OR respectively,

56
00:02:36.200 --> 00:02:38.840
we can formulate
the necessary equations to

57
00:02:38.840 --> 00:02:41.945
compute the 3D coordinates
of the point O.

58
00:02:41.945 --> 00:02:44.090
To make our computation easier,

59
00:02:44.090 --> 00:02:45.910
we will state some assumptions.

60
00:02:45.910 --> 00:02:48.470
First, we assume
that the two cameras

61
00:02:48.470 --> 00:02:51.935
used to construct the stereo
sensors are identical.

62
00:02:51.935 --> 00:02:54.020
Second, we will assume that while

63
00:02:54.020 --> 00:02:56.270
manufacturing the stereo sensor,

64
00:02:56.270 --> 00:02:58.130
we tried as hard as
possible to keep

65
00:02:58.130 --> 00:03:01.055
the two cameras
optical axes aligned.

66
00:03:01.055 --> 00:03:03.650
Let's now define
some important parameters

67
00:03:03.650 --> 00:03:05.225
of the stereo sensor.

68
00:03:05.225 --> 00:03:07.400
The focal length is, once again,

69
00:03:07.400 --> 00:03:09.710
the distance between
the camera center

70
00:03:09.710 --> 00:03:11.335
and the image plane.

71
00:03:11.335 --> 00:03:15.290
Second, the baseline is
defined as the distance along

72
00:03:15.290 --> 00:03:17.480
the shared x-axis between

73
00:03:17.480 --> 00:03:19.790
the left and
right camera centers.

74
00:03:19.790 --> 00:03:22.040
By defining a baseline
to represent

75
00:03:22.040 --> 00:03:23.420
the transformation between

76
00:03:23.420 --> 00:03:25.270
the two camera coordinate frames,

77
00:03:25.270 --> 00:03:29.060
we are assuming that the
rotation matrix is identity and

78
00:03:29.060 --> 00:03:31.520
there is only a
non-zero x component

79
00:03:31.520 --> 00:03:33.170
in the translation vector.

80
00:03:33.170 --> 00:03:35.990
The R and T
transformation therefore

81
00:03:35.990 --> 00:03:39.550
boils down to a single
baseline parameter B.

82
00:03:39.550 --> 00:03:43.055
Before proceeding, we will
project the previous figure

83
00:03:43.055 --> 00:03:46.430
to bird's eye view for
easier visualization.

84
00:03:46.430 --> 00:03:48.530
Now, let's define the quantities

85
00:03:48.530 --> 00:03:49.970
we would like to compute.

86
00:03:49.970 --> 00:03:52.910
We want to compute the
x and z coordinates of

87
00:03:52.910 --> 00:03:56.645
the point O with respect
to the left camera frame.

88
00:03:56.645 --> 00:03:59.060
The y coordinate can be estimated

89
00:03:59.060 --> 00:04:02.245
easily after the x and z
coordinates are computed.

90
00:04:02.245 --> 00:04:06.350
Remember, we are given
the baseline, focal length,

91
00:04:06.350 --> 00:04:08.330
and the coordinates
of the projection of

92
00:04:08.330 --> 00:04:12.110
the point O onto the left
and right image planes.

93
00:04:12.110 --> 00:04:14.690
We can see two similar triangles

94
00:04:14.690 --> 00:04:17.690
formed by the left
camera measurement as follows.

95
00:04:17.690 --> 00:04:19.790
The triangle formed by the depth

96
00:04:19.790 --> 00:04:22.700
z and the position
x is similar to

97
00:04:22.700 --> 00:04:25.225
the triangle formed
by the focal length f

98
00:04:25.225 --> 00:04:28.695
and the left measurement
x component xl.

99
00:04:28.695 --> 00:04:31.790
From this similarity we
can construct the equation

100
00:04:31.790 --> 00:04:35.300
z over equals x over xl.

101
00:04:35.300 --> 00:04:37.190
The same can be done for

102
00:04:37.190 --> 00:04:38.840
the right measurements but with

103
00:04:38.840 --> 00:04:41.375
the offset for
the baseline included.

104
00:04:41.375 --> 00:04:44.945
In this case, the two triangles
are defined by z,

105
00:04:44.945 --> 00:04:48.485
and the distance x minus
b and the focal length

106
00:04:48.485 --> 00:04:52.285
f and the right measurement
x component xr.

107
00:04:52.285 --> 00:04:55.820
Similarly, we can get
a second equation relating

108
00:04:55.820 --> 00:04:59.945
z to x via the right camera
parameters in measurements.

109
00:04:59.945 --> 00:05:01.850
From these two equations,

110
00:05:01.850 --> 00:05:05.540
we can now derive the
3D coordinates of the point O.

111
00:05:05.540 --> 00:05:09.830
We define the disparity d to
be the difference between

112
00:05:09.830 --> 00:05:11.450
the image coordinates of

113
00:05:11.450 --> 00:05:14.420
the same pixel in the
left and right images.

114
00:05:14.420 --> 00:05:17.570
We can easily transform
between image and pixel

115
00:05:17.570 --> 00:05:19.320
coordinates using the x and

116
00:05:19.320 --> 00:05:21.760
y offsets u Naught and v Naught.

117
00:05:21.760 --> 00:05:24.110
We then use
the two equations from

118
00:05:24.110 --> 00:05:26.240
the similar triangle relations to

119
00:05:26.240 --> 00:05:29.555
solve for the value
of z as follows.

120
00:05:29.555 --> 00:05:32.300
From there we use
the value of z to

121
00:05:32.300 --> 00:05:35.030
compute x with
the following expression.

122
00:05:35.030 --> 00:05:36.920
Finally, we can repeat

123
00:05:36.920 --> 00:05:38.900
the process in
the y direction with

124
00:05:38.900 --> 00:05:40.550
the same derivation to

125
00:05:40.550 --> 00:05:43.405
arrive at the following
expression for y.

126
00:05:43.405 --> 00:05:46.730
The three components of
the point position are now

127
00:05:46.730 --> 00:05:49.130
explicitly available
from the two sets

128
00:05:49.130 --> 00:05:51.470
of pixel measurements
available to us.

129
00:05:51.470 --> 00:05:54.560
Now that we have established
the equations needed for

130
00:05:54.560 --> 00:05:57.895
3D coordinate computation
from the stereo sensor,

131
00:05:57.895 --> 00:05:59.630
two problems arise to be

132
00:05:59.630 --> 00:06:01.865
able to perform this computation.

133
00:06:01.865 --> 00:06:03.800
First, we need to compute

134
00:06:03.800 --> 00:06:07.265
the focal length baseline
and x and y offsets.

135
00:06:07.265 --> 00:06:10.975
That is, we need to calibrate
the stereo camera system.

136
00:06:10.975 --> 00:06:14.645
Second, we need to find
the correspondence between

137
00:06:14.645 --> 00:06:17.555
each left and right
image pixel pair

138
00:06:17.555 --> 00:06:20.090
to be able to compute
their disparity.

139
00:06:20.090 --> 00:06:22.760
Fortunately, the
calibration problem can be

140
00:06:22.760 --> 00:06:25.495
solved using stereo
camera calibration.

141
00:06:25.495 --> 00:06:27.410
This is an extension of

142
00:06:27.410 --> 00:06:30.770
the monocular process we
discussed in the previous video,

143
00:06:30.770 --> 00:06:34.025
for which well-established
implementations are available.

144
00:06:34.025 --> 00:06:36.320
The correspondence
problem however,

145
00:06:36.320 --> 00:06:40.069
requires specialized algorithms
to efficiently perform

146
00:06:40.069 --> 00:06:42.230
the matching and
compute the disparity

147
00:06:42.230 --> 00:06:44.780
between left and
right image pixels,

148
00:06:44.780 --> 00:06:47.645
which we'll discuss
further in the next video.

149
00:06:47.645 --> 00:06:50.600
The output depth from
stereopsis suffers from

150
00:06:50.600 --> 00:06:52.370
some limitations particularly as

151
00:06:52.370 --> 00:06:54.550
points move further away
from the stereo camera.

152
00:06:54.550 --> 00:06:58.160
However, given a good disparity
estimation algorithm,

153
00:06:58.160 --> 00:06:59.480
the output is still useful for

154
00:06:59.480 --> 00:07:02.870
self-driving cars as a dense
source of depth information

155
00:07:02.870 --> 00:07:05.240
and closer range which exceeds

156
00:07:05.240 --> 00:07:08.180
the density we can get
from common Lidar sensors.

157
00:07:08.180 --> 00:07:09.920
To summarize this lesson,

158
00:07:09.920 --> 00:07:13.055
you've learned some historical
background on stereopsis.

159
00:07:13.055 --> 00:07:15.470
You also learned
the equations required to

160
00:07:15.470 --> 00:07:17.840
estimate 3D
coordinates of a pixel

161
00:07:17.840 --> 00:07:20.090
given the geometric
transformation between

162
00:07:20.090 --> 00:07:23.555
the two cameras sensors and
the disparity between pixels.

163
00:07:23.555 --> 00:07:25.430
In the next lesson we'll learn

164
00:07:25.430 --> 00:07:27.110
more about disparity generating

165
00:07:27.110 --> 00:07:30.380
algorithms and show
full examples on how to compute

166
00:07:30.380 --> 00:07:32.570
that disparity from
a stereo image pair

167
00:07:32.570 --> 00:07:36.300
using Python and OpenCV.
Will see you then.