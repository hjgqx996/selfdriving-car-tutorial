Self-driving cars require
accurate depth perception for the safe operation of
our autonomous vehicles. If you don't know how far away the cars are in front of you, how can you avoid
them while driving? Lidar and Radar sensors
are usually thought of as the primary 3D sensors
available for perception tasks. However, we can get
depth information from two or more cameras using
multi-view geometry. Specifically, we'll be describing the process of getting depth from two axis aligned cameras a setup known as
the stereo cameras. In this video, we will
cover the geometry of the stereo sensor
as well as how to derive the 3D coordinates
of a point given its projection onto two images
of the stereo sensor. Stereopsis, the process
of stereo vision, was first described by
Charles Wheatstone back in 1838. He recognized that because each eye views
the visual world from a slightly different
horizontal position that each eye's image
differs from the other. Objects at different
distances from the eye project images into
the two eyes that differ in their horizontal position
giving depth cues of horizontal disparity that are also known as
binocular disparity. However, historical
evidence suggests that stereopsis was discovered
much earlier than this. In fact some drawings
by Leonardo da Vinci depict accurate geometry of
depth through stereopsis. Up to the 19th century, the phenomenon of stereopsis was primarily used for entertainment. Anaglyphs were used to provide a stereoscopic 3D effect when
viewed with 2D color glass, where each lens employs different chromatically
opposite colors, usually red and cyan. Nowadays, we use stereopsis with complex algorithms to derive depth from two images using a similar concept to
Da Vincis drawings. Now, let us delve into
the geometry of a stereo sensor. A stereo sensor is
usually created by two cameras with
parallel optical axes. To simplify
the problem even more, most manufacturers align
the cameras in 3D space so that the two image
planes are aligned with only an offset in the x-axis. Given a known rotation
and translation between the two cameras
and a known projection of a point O in 3D to
the two camera frames resulting in pixel locations
OL and OR respectively, we can formulate
the necessary equations to compute the 3D coordinates
of the point O. To make our computation easier, we will state some assumptions. First, we assume
that the two cameras used to construct the stereo
sensors are identical. Second, we will assume that while manufacturing the stereo sensor, we tried as hard as
possible to keep the two cameras
optical axes aligned. Let's now define
some important parameters of the stereo sensor. The focal length is, once again, the distance between
the camera center and the image plane. Second, the baseline is
defined as the distance along the shared x-axis between the left and
right camera centers. By defining a baseline
to represent the transformation between the two camera coordinate frames, we are assuming that the
rotation matrix is identity and there is only a
non-zero x component in the translation vector. The R and T
transformation therefore boils down to a single
baseline parameter B. Before proceeding, we will
project the previous figure to bird's eye view for
easier visualization. Now, let's define the quantities we would like to compute. We want to compute the
x and z coordinates of the point O with respect
to the left camera frame. The y coordinate can be estimated easily after the x and z
coordinates are computed. Remember, we are given
the baseline, focal length, and the coordinates
of the projection of the point O onto the left
and right image planes. We can see two similar triangles formed by the left
camera measurement as follows. The triangle formed by the depth z and the position
x is similar to the triangle formed
by the focal length f and the left measurement
x component xl. From this similarity we
can construct the equation z over equals x over xl. The same can be done for the right measurements but with the offset for
the baseline included. In this case, the two triangles
are defined by z, and the distance x minus
b and the focal length f and the right measurement
x component xr. Similarly, we can get
a second equation relating z to x via the right camera
parameters in measurements. From these two equations, we can now derive the
3D coordinates of the point O. We define the disparity d to
be the difference between the image coordinates of the same pixel in the
left and right images. We can easily transform
between image and pixel coordinates using the x and y offsets u Naught and v Naught. We then use
the two equations from the similar triangle relations to solve for the value
of z as follows. From there we use
the value of z to compute x with
the following expression. Finally, we can repeat the process in
the y direction with the same derivation to arrive at the following
expression for y. The three components of
the point position are now explicitly available
from the two sets of pixel measurements
available to us. Now that we have established
the equations needed for 3D coordinate computation
from the stereo sensor, two problems arise to be able to perform this computation. First, we need to compute the focal length baseline
and x and y offsets. That is, we need to calibrate
the stereo camera system. Second, we need to find
the correspondence between each left and right
image pixel pair to be able to compute
their disparity. Fortunately, the
calibration problem can be solved using stereo
camera calibration. This is an extension of the monocular process we
discussed in the previous video, for which well-established
implementations are available. The correspondence
problem however, requires specialized algorithms
to efficiently perform the matching and
compute the disparity between left and
right image pixels, which we'll discuss
further in the next video. The output depth from
stereopsis suffers from some limitations particularly as points move further away
from the stereo camera. However, given a good disparity
estimation algorithm, the output is still useful for self-driving cars as a dense
source of depth information and closer range which exceeds the density we can get
from common Lidar sensors. To summarize this lesson, you've learned some historical
background on stereopsis. You also learned
the equations required to estimate 3D
coordinates of a pixel given the geometric
transformation between the two cameras sensors and
the disparity between pixels. In the next lesson we'll learn more about disparity generating algorithms and show
full examples on how to compute that disparity from
a stereo image pair using Python and OpenCV.
Will see you then.